{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chall3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "VEL0fHP0_nKG"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicologhielmetti/AN2DL-challenges/blob/master/challenge3/chall3.ipynb\" target=\"_parent\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZpmn88cRoE8"
      },
      "source": [
        "import json, os\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import random\n",
        "import keras.layers as layers\n",
        "import keras.models as models\n",
        "from keras.initializers import orthogonal\n",
        "from keras.optimizers import Adam\n",
        "import shutil"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2fTIh98Z_nKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c547d5-f35f-4e30-914f-5f8fbaf88dfc"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiIPlGdaIKiW",
        "outputId": "4a803236-1e61-4f4d-ae2f-74a4f71b5e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlhygyiVnN7-",
        "outputId": "9dd51fa9-34b8-4a20-8e8d-62f5fa089a76"
      },
      "source": [
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1tglwr5cbQbzrSLmJlHmz33htFUw0yzc4\n",
        "!unzip -qq /content/anndl-2020-vqa.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tglwr5cbQbzrSLmJlHmz33htFUw0yzc4\n",
            "To: /content/anndl-2020-vqa.zip\n",
            "4.26GB [01:16, 56.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rOgcNM3i_nKS"
      },
      "source": [
        "#!unzip -qq VQA_Dataset.zip -d VQA_Dataset"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siaKwgfhgtOk"
      },
      "source": [
        "def create_split_files(file_path, train_val_split):\n",
        "  with open(file_path,'r') as json_dataset:\n",
        "    data = json.load(json_dataset)\n",
        "\n",
        "    tot_list = []\n",
        "    for k,v in data.items():\n",
        "      tot_list.append({k:v})\n",
        "    \n",
        "    train_list = random.sample(tot_list, int(len(tot_list) * (1 - train_val_split)))\n",
        "    validation_list = [x for x in tot_list if x not in train_list]\n",
        "\n",
        "    with open(\"VQA_Dataset/train.json\", \"w\") as train:\n",
        "        json.dump(train_list, train)\n",
        "        train.close()\n",
        "    with open(\"VQA_Dataset/valid.json\", \"w\") as validation:\n",
        "        json.dump(validation_list, validation)\n",
        "        validation.close()\n",
        "    \n",
        "    #assert len([x for x in tot_list if x in train_list and x in validation_list]) == 0\n",
        "\n",
        "def create_train_test_dirs(json_definition, dataset_path, split_name):\n",
        "    dest_dir = os.path.join(dataset_path, split_name)\n",
        "    if not os.path.isdir(dest_dir):\n",
        "      os.mkdir(dest_dir)\n",
        "      os.mkdir(os.path.join(dest_dir, split_name))\n",
        "    if split_name == 'test':\n",
        "      for k,v in json_definition.items():\n",
        "        try:\n",
        "            shutil.move(\n",
        "                os.path.join(dataset_path, \"Images\", v['image_id']+'.png'),\n",
        "                os.path.join(dest_dir, split_name, v['image_id']+'.png')\n",
        "            )\n",
        "        except FileNotFoundError as e:\n",
        "            print(\"Split name: \" + split_name + \". File not found: \" + str(e))\n",
        "            continue\n",
        "        \n",
        "    else:\n",
        "      for elem in json_definition:\n",
        "          #print(os.path.join(dataset_path, \"Images\", elem[list(elem.keys())[0]]['image_id'][:-1]+'.png'))\n",
        "          try:\n",
        "              shutil.move(\n",
        "                  os.path.join(dataset_path, \"Images\", elem[list(elem.keys())[0]]['image_id']+'.png'),\n",
        "                  os.path.join(dest_dir, split_name, elem[list(elem.keys())[0]]['image_id']+'.png')\n",
        "              )\n",
        "          except FileNotFoundError as e:\n",
        "              print(\"Split name: \" + split_name + \". File not found: \" + str(e))\n",
        "              continue\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MRoHGbaRwCD"
      },
      "source": [
        "img_size = (256, 256)\n",
        "\n",
        "random.seed(96)\n",
        "\n",
        "cwd = os.getcwd()\n",
        "datasetName = os.path.join(cwd,'VQA_Dataset')\n",
        "trainJsonName = 'train.json'\n",
        "validJsonName = 'valid.json'\n",
        "testJsonName  = 'test_questions.json'\n",
        "#os.chdir(datasetName)\n",
        "imagesPath = os.path.join(datasetName, 'Images')\n",
        "trainJsonPath = os.path.join(datasetName, trainJsonName)\n",
        "validJsonPath = os.path.join(datasetName, validJsonName)\n",
        "testJsonPath  = os.path.join(datasetName, testJsonName)\n",
        "\n",
        "\n",
        "create_split_files(os.path.join(datasetName, 'train_questions_annotations.json'), 0.5)\n",
        "\n",
        "with open(trainJsonPath,'r') as json_file_train, open(validJsonPath, 'r') as json_file_valid, open(testJsonPath, 'r') as json_file_test:\n",
        "    data_train = json.load(json_file_train)\n",
        "    data_valid = json.load(json_file_valid)\n",
        "    data_test = json.load(json_file_test)\n",
        "\n",
        "    json_file_train.close()\n",
        "    json_file_valid.close()\n",
        "    json_file_test.close()\n",
        "    \n",
        "os.chdir(cwd)\n",
        "create_train_test_dirs(data_train, datasetName, 'train')\n",
        "create_train_test_dirs(data_valid, datasetName, 'validation')\n",
        "create_train_test_dirs(data_test, datasetName, 'test')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FYFYGDfh_nKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a0a866-f556-43bb-bad1-8476be213b1d"
      },
      "source": [
        "preproc_fun_fixed = partial(tf.keras.preprocessing.image.smart_resize, size=img_size)\n",
        "batch_size = 32\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, data_format='channels_last', preprocessing_function=preproc_fun_fixed)\n",
        "train_data = datagen.flow_from_directory(datasetName+'/train', img_size, class_mode='input', batch_size=batch_size)\n",
        "valid_data = datagen.flow_from_directory(datasetName+'/validation', img_size, class_mode='input', batch_size=batch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 20833 images belonging to 1 classes.\n",
            "Found 7911 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUWJZpBuSYBJ"
      },
      "source": [
        "def Conv2DLayer(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n",
        "    prefix = f'block_{block_id}_'\n",
        "    x = layers.Conv2D(filters, kernel_size=kernel, strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_init, name=prefix+'conv')(x)\n",
        "    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n",
        "    x = layers.Dropout(0.2, name=prefix+'drop')((x))\n",
        "    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n",
        "    return x\n",
        "\n",
        "def Transpose_Conv2D(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n",
        "    prefix = f'block_{block_id}_'\n",
        "    x = layers.Conv2DTranspose(filters, kernel_size=kernel, strides=strides, padding=padding,\n",
        "                               kernel_initializer=kernel_init, name=prefix+'de-conv')(x)\n",
        "    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n",
        "    x = layers.Dropout(0.2, name=prefix+'drop')((x))\n",
        "    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def AutoEncoder(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    \n",
        "    # 256 x 256\n",
        "    conv1 = Conv2DLayer(inputs, 64, 3, strides=1, padding='same', block_id=1)\n",
        "    conv2 = Conv2DLayer(conv1, 64, 3, strides=2, padding='same', block_id=2)\n",
        "    \n",
        "    # 128 x 128\n",
        "    conv3 = Conv2DLayer(conv2, 128, 5, strides=2, padding='same', block_id=3)\n",
        "    \n",
        "    # 64 x 64\n",
        "    conv4 = Conv2DLayer(conv3, 128, 3, strides=1, padding='same', block_id=4)\n",
        "    conv5 = Conv2DLayer(conv4, 256, 5, strides=2, padding='same', block_id=5)\n",
        "    \n",
        "    # 32 x 32\n",
        "    conv6 = Conv2DLayer(conv5, 512, 3, strides=2, padding='same', block_id=6)\n",
        "    \n",
        "    # 16 x 16\n",
        "    deconv1 = Transpose_Conv2D(conv6, 512, 3, strides=2, padding='same', block_id=7)\n",
        "    \n",
        "    # 32 x 32\n",
        "    skip1 = layers.concatenate([deconv1, conv5], name='skip1')\n",
        "    conv7 = Conv2DLayer(skip1, 256, 3, strides=1, padding='same', block_id=8)\n",
        "    deconv2 = Transpose_Conv2D(conv7, 128, 3, strides=2, padding='same', block_id=9)\n",
        "    \n",
        "    # 64 x 64\n",
        "    skip2 = layers.concatenate([deconv2, conv3], name='skip2')\n",
        "    conv8 = Conv2DLayer(skip2, 128, 5, strides=1, padding='same', block_id=10)\n",
        "    deconv3 = Transpose_Conv2D(conv8, 64, 3, strides=2, padding='same', block_id=11)\n",
        "    \n",
        "    # 128 x 128\n",
        "    skip3 = layers.concatenate([deconv3, conv2], name='skip3')\n",
        "    conv9 = Conv2DLayer(skip3, 64, 5, strides=1, padding='same', block_id=12)\n",
        "    deconv4 = Transpose_Conv2D(conv9, 64, 3, strides=2, padding='same', block_id=13)\n",
        "    \n",
        "    # 256 x 256\n",
        "    skip3 = layers.concatenate([deconv4, conv1])\n",
        "    conv10 = layers.Conv2D(3, 3, strides=1, padding='same', activation='sigmoid',\n",
        "                       kernel_initializer=orthogonal(), name='final_conv')(skip3)\n",
        "\n",
        "    \n",
        "    return models.Model(inputs=inputs, outputs=conv10)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUkfFm5w2iis"
      },
      "source": [
        "autoencoder = AutoEncoder((*img_size, 3))\n",
        "model_opt = Adam(lr=0.002)\n",
        "\n",
        "autoencoder.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
        "#autoencoder.summary()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7sT-cwsh8_Z"
      },
      "source": [
        "saved_weight = os.path.join('/content/drive/MyDrive','saved_models_chall3', 'dataweights.{epoch:02d}.hdf5')\n",
        "modelchk = tf.keras.callbacks.ModelCheckpoint(saved_weight, \n",
        "                                              monitor='val_loss', \n",
        "                                              verbose=1,\n",
        "                                              save_best_only=True, \n",
        "                                              save_weights_only=False\n",
        "                                              )\n",
        "\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='logs',\n",
        "                                          histogram_freq=0,\n",
        "                                          write_graph=True,\n",
        "                                          write_images=True\n",
        "                                          )\n",
        "\n",
        "csv_logger = tf.keras.callbacks.CSVLogger('logs/keras_log.csv',\n",
        "                                       append=True)\n",
        "\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7vfj-TTZE4U"
      },
      "source": [
        "autoencoder = tf.keras.models.load_model(os.path.join('/content/drive/MyDrive','saved_models_chall3', 'dataweights.02.hdf5'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96RbK1ADjEpD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "9ee6102d-9e51-4d02-86de-79e760b99be5"
      },
      "source": [
        "epochs=50\n",
        "autoencoder.fit(train_data,\n",
        "                steps_per_epoch = len(train_data),\n",
        "                epochs=epochs,\n",
        "                verbose=1,\n",
        "                validation_data=valid_data,\n",
        "                validation_steps = len(train_data),\n",
        "                callbacks=[modelchk,tensorboard, csv_logger, es_callback]\n",
        "                )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "652/652 [==============================] - ETA: 0s - loss: 7.3180e-04 - accuracy: 0.9569WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 652 batches). You may need to use the repeat() function when building your dataset.\n",
            "652/652 [==============================] - 700s 1s/step - loss: 7.3180e-04 - accuracy: 0.9569 - val_loss: 5.4506e-04 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00055, saving model to /content/drive/MyDrive/saved_models_chall3/dataweights.01.hdf5\n",
            "Epoch 2/50\n",
            "652/652 [==============================] - 560s 859ms/step - loss: 6.9260e-04 - accuracy: 0.9582\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-66f05b18f3de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodelchk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   2637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2639\u001b[0;31m     \u001b[0mrow_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2640\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2641\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2639\u001b[0;31m     \u001b[0mrow_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2640\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2641\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
          ]
        }
      ]
    }
  ]
}