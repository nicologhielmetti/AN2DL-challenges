{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "chall3.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyOBiDD/20CyzK8OuCI1ZRS7"
  },
  "kernelspec": {
   "name": "python37964bitan2dlkaggleconda6a00685a482b4c38a65fd0ed5651f3bd",
   "language": "python",
   "display_name": "Python 3.7.9 64-bit ('AN2DL-kaggle': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/nicologhielmetti/AN2DL-challenges/blob/master/challenge3/chall3.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kZpmn88cRoE8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1611691686193,
     "user_tz": -60,
     "elapsed": 2588,
     "user": {
      "displayName": "Nicolò Ghielmetti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgV3M0RDZIyrC4Ib1aK7LP0q2aCu3pzwX1sXRb-=s64",
      "userId": "11140493505689836770"
     }
    }
   },
   "source": [
    "import json, os\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras.layers as layers\n",
    "import keras.models as models\n",
    "from keras.initializers import orthogonal\n",
    "from keras.optimizers import Adam\n",
    "import shutil"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlhygyiVnN7-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1611691885427,
     "user_tz": -60,
     "elapsed": 201791,
     "user": {
      "displayName": "Nicolò Ghielmetti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgV3M0RDZIyrC4Ib1aK7LP0q2aCu3pzwX1sXRb-=s64",
      "userId": "11140493505689836770"
     }
    },
    "outputId": "80a69c7d-da14-4b32-a9b8-01ba0d390dd3"
   },
   "source": [
    "#!pip install gdown\n",
    "#!gdown https://drive.google.com/uc?id=1tglwr5cbQbzrSLmJlHmz33htFUw0yzc4\n",
    "#!unzip -qq /content/anndl-2020-vqa.zip"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "!unzip -qq VQA_Dataset.zip -d VQA_Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "siaKwgfhgtOk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1611691885431,
     "user_tz": -60,
     "elapsed": 201772,
     "user": {
      "displayName": "Nicolò Ghielmetti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgV3M0RDZIyrC4Ib1aK7LP0q2aCu3pzwX1sXRb-=s64",
      "userId": "11140493505689836770"
     }
    }
   },
   "source": [
    "def create_split_files(file_path, train_val_split):\n",
    "    with open(file_path,'r') as json_dataset:\n",
    "        data = json.load(json_dataset)\n",
    "\n",
    "    tot_list = list()\n",
    "    for k,v in data.items():\n",
    "        tot_list.append(v)\n",
    "\n",
    "    train_list = random.sample(tot_list, int(len(tot_list) * (1 - train_val_split)))\n",
    "    validation_list = [i for i in tot_list if i not in train_list]\n",
    "\n",
    "    with open(\"VQA_Dataset/train.json\", \"w\") as train:\n",
    "        json.dump(train_list, train)\n",
    "        train.close()\n",
    "    with open(\"VQA_Dataset/valid.json\", \"w\") as validation:\n",
    "        json.dump(validation_list, validation)\n",
    "        validation.close()\n",
    "    \n",
    "    #assert len([x for x in tot_list if x in train_list and x in validation_list]) == 0\n",
    "\n",
    "def generate_correct_test_file(testJsonPath):\n",
    "    with open(testJsonPath,'r') as json_dataset:\n",
    "        data = json.load(json_dataset)\n",
    "\n",
    "    test_list = list()\n",
    "    for k,v in data.items():\n",
    "        test_list.append(v)\n",
    "\n",
    "    with open(\"VQA_Dataset/test.json\", \"w\") as test:\n",
    "        json.dump(test_list, test)\n",
    "        test.close()\n",
    "\n",
    "def create_train_test_dirs(json_definition, dataset_path, split_name):\n",
    "    dest_dir = os.path.join(dataset_path, split_name)\n",
    "    if not os.path.isdir(dest_dir):\n",
    "      os.mkdir(dest_dir)\n",
    "      os.mkdir(os.path.join(dest_dir, split_name))\n",
    "    for k,v in json_definition.items():\n",
    "        try:\n",
    "            shutil.copy(\n",
    "                os.path.join(dataset_path, \"Images\", k +'.png'),\n",
    "                os.path.join(dest_dir, split_name, k +'.png')\n",
    "            )\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"Split name: \" + split_name + \". File not found: \" + str(e))\n",
    "            continue\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6MRoHGbaRwCD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1611692217793,
     "user_tz": -60,
     "elapsed": 241726,
     "user": {
      "displayName": "Nicolò Ghielmetti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgV3M0RDZIyrC4Ib1aK7LP0q2aCu3pzwX1sXRb-=s64",
      "userId": "11140493505689836770"
     }
    },
    "outputId": "12a6437e-2ab5-4295-b46e-af4118d670a0"
   },
   "source": [
    "\n",
    "random.seed(96)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "datasetName = os.path.join(cwd,'VQA_Dataset')\n",
    "trainJsonName = 'train.json'\n",
    "validJsonName = 'valid.json'\n",
    "testJsonName  = 'test_questions.json'\n",
    "testJsonNameCorrect = 'test.json'\n",
    "imagesPath = os.path.join(datasetName, 'Images')\n",
    "trainJsonPath = os.path.join(datasetName, trainJsonName)\n",
    "validJsonPath = os.path.join(datasetName, validJsonName)\n",
    "testJsonPath  = os.path.join(datasetName, testJsonName)\n",
    "testJsonPathCorrect  = os.path.join(datasetName, testJsonNameCorrect)\n",
    "\n",
    "create_split_files(os.path.join(datasetName, 'train_questions_annotations.json'), 0.3)\n",
    "generate_correct_test_file(testJsonPath)\n",
    "\n",
    "with open(trainJsonPath,'r') as json_file_train, open(validJsonPath, 'r') as json_file_valid, open(testJsonPathCorrect, 'r') as json_file_test:\n",
    "    data_train = json.load(json_file_train)\n",
    "    data_valid = json.load(json_file_valid)\n",
    "    data_test = json.load(json_file_test)\n",
    "\n",
    "    json_file_train.close()\n",
    "    json_file_valid.close()\n",
    "    json_file_test.close()\n",
    "\n",
    "\n",
    "os.chdir(cwd)\n",
    "#create_train_test_dirs(data_train, datasetName, 'train')\n",
    "#create_train_test_dirs(data_valid, datasetName, 'validation')\n",
    "#create_train_test_dirs(data_test, datasetName, 'test')"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nfor \\ntry:\\n    shutil.copy(\\n        os.path.join(imagesPath, k +\\'.png\\'),\\n        os.path.join(dest_dir, split_name, k +\\'.png\\')\\n    )\\nexcept FileNotFoundError as e:\\n    print(\"Split name: \" + split_name + \". File not found: \" + str(e))\\n    continue\\n'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_dir = os.path.join(datasetName, \"ImagesExt\")\n",
    "if not os.path.isdir(dest_dir):\n",
    "  os.mkdir(dest_dir)\n",
    "shutil.move(imagesPath, dest_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20534 images belonging to 1 classes.\n",
      "Found 8799 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "img_size = (256, 256)\n",
    "preproc_fun_fixed = partial(tf.keras.preprocessing.image.smart_resize, size=img_size)\n",
    "batch_size = 32\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                          data_format='channels_last',\n",
    "                                                          preprocessing_function=preproc_fun_fixed,\n",
    "                                                          validation_split=0.3)\n",
    "train_data = datagen.flow_from_directory(datasetName+'/ImagesExt', img_size, class_mode='input',\n",
    "                                         batch_size=batch_size, subset=\"training\")\n",
    "valid_data = datagen.flow_from_directory(datasetName+'/ImagesExt', img_size, class_mode='input',\n",
    "                                         batch_size=batch_size, subset=\"validation\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hUWJZpBuSYBJ"
   },
   "source": [
    "def Conv2DLayer(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n",
    "    prefix = f'block_{block_id}_'\n",
    "    x = layers.Conv2D(filters, kernel_size=kernel, strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_init, name=prefix+'conv')(x)\n",
    "    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n",
    "    x = layers.Dropout(0.2, name=prefix+'drop')((x))\n",
    "    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n",
    "    return x\n",
    "\n",
    "def Transpose_Conv2D(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):\n",
    "    prefix = f'block_{block_id}_'\n",
    "    x = layers.Conv2DTranspose(filters, kernel_size=kernel, strides=strides, padding=padding,\n",
    "                               kernel_initializer=kernel_init, name=prefix+'de-conv')(x)\n",
    "    x = layers.LeakyReLU(name=prefix+'lrelu')(x)\n",
    "    x = layers.Dropout(0.2, name=prefix+'drop')((x))\n",
    "    x = layers.BatchNormalization(name=prefix+'conv_bn')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def AutoEncoder(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # 256 x 256\n",
    "    conv1 = Conv2DLayer(inputs, 64, 3, strides=1, padding='same', block_id=1)\n",
    "    conv2 = Conv2DLayer(conv1, 64, 3, strides=2, padding='same', block_id=2)\n",
    "    \n",
    "    # 128 x 128\n",
    "    conv3 = Conv2DLayer(conv2, 128, 5, strides=2, padding='same', block_id=3)\n",
    "    \n",
    "    # 64 x 64\n",
    "    conv4 = Conv2DLayer(conv3, 128, 3, strides=1, padding='same', block_id=4)\n",
    "    conv5 = Conv2DLayer(conv4, 256, 5, strides=2, padding='same', block_id=5)\n",
    "    \n",
    "    # 32 x 32\n",
    "    conv6 = Conv2DLayer(conv5, 512, 3, strides=2, padding='same', block_id=6)\n",
    "    \n",
    "    # 16 x 16\n",
    "    deconv1 = Transpose_Conv2D(conv6, 512, 3, strides=2, padding='same', block_id=7)\n",
    "    \n",
    "    # 32 x 32\n",
    "    skip1 = layers.concatenate([deconv1, conv5], name='skip1')\n",
    "    conv7 = Conv2DLayer(skip1, 256, 3, strides=1, padding='same', block_id=8)\n",
    "    deconv2 = Transpose_Conv2D(conv7, 128, 3, strides=2, padding='same', block_id=9)\n",
    "    \n",
    "    # 64 x 64\n",
    "    skip2 = layers.concatenate([deconv2, conv3], name='skip2')\n",
    "    conv8 = Conv2DLayer(skip2, 128, 5, strides=1, padding='same', block_id=10)\n",
    "    deconv3 = Transpose_Conv2D(conv8, 64, 3, strides=2, padding='same', block_id=11)\n",
    "    \n",
    "    # 128 x 128\n",
    "    skip3 = layers.concatenate([deconv3, conv2], name='skip3')\n",
    "    conv9 = Conv2DLayer(skip3, 64, 5, strides=1, padding='same', block_id=12)\n",
    "    deconv4 = Transpose_Conv2D(conv9, 64, 3, strides=2, padding='same', block_id=13)\n",
    "    \n",
    "    # 256 x 256\n",
    "    skip3 = layers.concatenate([deconv4, conv1])\n",
    "    conv10 = layers.Conv2D(3, 3, strides=1, padding='same', activation='sigmoid',\n",
    "                       kernel_initializer=orthogonal(), name='final_conv')(skip3)\n",
    "\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=conv10)"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yUkfFm5w2iis"
   },
   "source": [
    "autoencoder = AutoEncoder((*img_size, 3))\n",
    "model_opt = Adam(lr=0.002)\n",
    "\n",
    "autoencoder.compile(optimizer=model_opt, loss='mse', metrics=['accuracy'])\n",
    "autoencoder.summary()"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block_1_conv (Conv2D)           (None, 256, 256, 64) 1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block_1_lrelu (LeakyReLU)       (None, 256, 256, 64) 0           block_1_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1_drop (Dropout)          (None, 256, 256, 64) 0           block_1_lrelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1_conv_bn (BatchNormaliza (None, 256, 256, 64) 256         block_1_drop[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_2_conv (Conv2D)           (None, 128, 128, 64) 36928       block_1_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_lrelu (LeakyReLU)       (None, 128, 128, 64) 0           block_2_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_2_drop (Dropout)          (None, 128, 128, 64) 0           block_2_lrelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2_conv_bn (BatchNormaliza (None, 128, 128, 64) 256         block_2_drop[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_3_conv (Conv2D)           (None, 64, 64, 128)  204928      block_2_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3_lrelu (LeakyReLU)       (None, 64, 64, 128)  0           block_3_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_3_drop (Dropout)          (None, 64, 64, 128)  0           block_3_lrelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3_conv_bn (BatchNormaliza (None, 64, 64, 128)  512         block_3_drop[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_4_conv (Conv2D)           (None, 64, 64, 128)  147584      block_3_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_lrelu (LeakyReLU)       (None, 64, 64, 128)  0           block_4_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_4_drop (Dropout)          (None, 64, 64, 128)  0           block_4_lrelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4_conv_bn (BatchNormaliza (None, 64, 64, 128)  512         block_4_drop[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_5_conv (Conv2D)           (None, 32, 32, 256)  819456      block_4_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_lrelu (LeakyReLU)       (None, 32, 32, 256)  0           block_5_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_5_drop (Dropout)          (None, 32, 32, 256)  0           block_5_lrelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_5_conv_bn (BatchNormaliza (None, 32, 32, 256)  1024        block_5_drop[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_6_conv (Conv2D)           (None, 16, 16, 512)  1180160     block_5_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_6_lrelu (LeakyReLU)       (None, 16, 16, 512)  0           block_6_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_6_drop (Dropout)          (None, 16, 16, 512)  0           block_6_lrelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_6_conv_bn (BatchNormaliza (None, 16, 16, 512)  2048        block_6_drop[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_7_de-conv (Conv2DTranspos (None, 32, 32, 512)  2359808     block_6_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_lrelu (LeakyReLU)       (None, 32, 32, 512)  0           block_7_de-conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_drop (Dropout)          (None, 32, 32, 512)  0           block_7_lrelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_7_conv_bn (BatchNormaliza (None, 32, 32, 512)  2048        block_7_drop[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "skip1 (Concatenate)             (None, 32, 32, 768)  0           block_7_conv_bn[0][0]            \n",
      "                                                                 block_5_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_conv (Conv2D)           (None, 32, 32, 256)  1769728     skip1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_8_lrelu (LeakyReLU)       (None, 32, 32, 256)  0           block_8_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_8_drop (Dropout)          (None, 32, 32, 256)  0           block_8_lrelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_8_conv_bn (BatchNormaliza (None, 32, 32, 256)  1024        block_8_drop[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_9_de-conv (Conv2DTranspos (None, 64, 64, 128)  295040      block_8_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_lrelu (LeakyReLU)       (None, 64, 64, 128)  0           block_9_de-conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_drop (Dropout)          (None, 64, 64, 128)  0           block_9_lrelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_9_conv_bn (BatchNormaliza (None, 64, 64, 128)  512         block_9_drop[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "skip2 (Concatenate)             (None, 64, 64, 256)  0           block_9_conv_bn[0][0]            \n",
      "                                                                 block_3_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_conv (Conv2D)          (None, 64, 64, 128)  819328      skip2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_lrelu (LeakyReLU)      (None, 64, 64, 128)  0           block_10_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_10_drop (Dropout)         (None, 64, 64, 128)  0           block_10_lrelu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_10_conv_bn (BatchNormaliz (None, 64, 64, 128)  512         block_10_drop[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_11_de-conv (Conv2DTranspo (None, 128, 128, 64) 73792       block_10_conv_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_lrelu (LeakyReLU)      (None, 128, 128, 64) 0           block_11_de-conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_drop (Dropout)         (None, 128, 128, 64) 0           block_11_lrelu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_11_conv_bn (BatchNormaliz (None, 128, 128, 64) 256         block_11_drop[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "skip3 (Concatenate)             (None, 128, 128, 128 0           block_11_conv_bn[0][0]           \n",
      "                                                                 block_2_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_conv (Conv2D)          (None, 128, 128, 64) 204864      skip3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_lrelu (LeakyReLU)      (None, 128, 128, 64) 0           block_12_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_12_drop (Dropout)         (None, 128, 128, 64) 0           block_12_lrelu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_12_conv_bn (BatchNormaliz (None, 128, 128, 64) 256         block_12_drop[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_13_de-conv (Conv2DTranspo (None, 256, 256, 64) 36928       block_12_conv_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_13_lrelu (LeakyReLU)      (None, 256, 256, 64) 0           block_13_de-conv[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_13_drop (Dropout)         (None, 256, 256, 64) 0           block_13_lrelu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_13_conv_bn (BatchNormaliz (None, 256, 256, 64) 256         block_13_drop[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 256, 128 0           block_13_conv_bn[0][0]           \n",
      "                                                                 block_1_conv_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv2D)             (None, 256, 256, 3)  3459        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,963,267\n",
      "Trainable params: 7,958,531\n",
      "Non-trainable params: 4,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K7sT-cwsh8_Z"
   },
   "source": [
    "saved_weight = os.path.join('/content/drive/MyDrive','saved_models_chall3', 'dataweights.{epoch:02d}.hdf5')\n",
    "modelchk = tf.keras.callbacks.ModelCheckpoint(saved_weight,\n",
    "                                              monitor='val_loss',\n",
    "                                              verbose=1,\n",
    "                                              save_best_only=True,\n",
    "                                              save_weights_only=False\n",
    "                                              )\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='logs',\n",
    "                                          histogram_freq=0,\n",
    "                                          write_graph=True,\n",
    "                                          write_images=True\n",
    "                                          )\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('logs/keras_log.csv',\n",
    "                                       append=True)\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autoencoder = tf.keras.models.load_model(os.path.join('/content/drive/MyDrive','saved_models_chall3', 'dataweights.02.hdf5'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "96RbK1ADjEpD"
   },
   "source": [
    "epochs=50\n",
    "autoencoder.fit(train_data,\n",
    "                steps_per_epoch = len(train_data),\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=valid_data,\n",
    "                validation_steps = len(train_data),\n",
    "                callbacks=[modelchk,tensorboard, csv_logger, es_callback]\n",
    "                )"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ]
  }
 ]
}