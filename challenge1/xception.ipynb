{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di xception.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "axkVKeQdPt2r"
      },
      "source": [
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1Mv7vKoI-QL6kV-1TIDE7N67_L0LXvJAg\n",
        "!unzip /content/ANDL2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8jkqYB8P4oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2bdfe19-b328-4b06-bc0a-abef5c60ff13"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsD5w4czQi_g"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorboard import program\n",
        "\n",
        "SEED = 1996"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch9vSkitQZWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582fbd1a-50d7-4125-dca0-6714bab72b39"
      },
      "source": [
        "# Batch size\n",
        "bs = 32\n",
        "\n",
        "# img shape\n",
        "img_h = 256\n",
        "img_w = 256\n",
        "\n",
        "num_classes=3\n",
        "# Load Xception Model\n",
        "Xception = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0QPsNqaQ5Y8"
      },
      "source": [
        "Xception.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGrDoIjHQ9NS"
      },
      "source": [
        "train_path = os.path.join(os.getcwd(), 'MaskDataset/training')\n",
        "test_path  = os.path.join(os.getcwd(), 'MaskDataset/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BEC_wA9RKp7"
      },
      "source": [
        "def divideDatasetInTargetFolders(json_definition, dataset_path):\n",
        "    for elem in json_definition:\n",
        "        dest_dir = os.path.join(dataset_path, str(json_definition[elem]))\n",
        "        if not os.path.isdir(dest_dir):\n",
        "            os.mkdir(dest_dir)\n",
        "        try:\n",
        "            shutil.move(os.path.join(dataset_path, elem),\n",
        "                        os.path.join(dest_dir, elem)\n",
        "                        )\n",
        "        except FileNotFoundError as e:\n",
        "            print(\"File not found: \" + str(e))\n",
        "            continue\n",
        "    os.mkdir(os.path.join(dataset_path, \"augmented\"))\n",
        "    os.mkdir(os.path.join(dataset_path, \"augmented/training\"))\n",
        "    os.mkdir(os.path.join(dataset_path, \"augmented/validation\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U15-qJlFRD7p"
      },
      "source": [
        "division_dict = json.load(\n",
        "  open(os.path.join(os.getcwd(), 'MaskDataset/train_gt.json'))\n",
        ")\n",
        "\n",
        "divideDatasetInTargetFolders(division_dict, train_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex5NDRY5RGVV"
      },
      "source": [
        "preproc_fun_fixed = partial(tf.keras.preprocessing.image.smart_resize, size=(img_w, img_h))\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rotation_range=10,\n",
        "                                    width_shift_range=10,\n",
        "                                    height_shift_range=10,\n",
        "                                    zoom_range=0.3,\n",
        "                                    horizontal_flip=True,\n",
        "                                    fill_mode='reflect',\n",
        "                                    rescale=1. / 255,\n",
        "                                    validation_split=0.3,\n",
        "                                    preprocessing_function=preproc_fun_fixed\n",
        "                                    )\n",
        "\n",
        "test_data_gen = ImageDataGenerator(rescale=1. / 255, preprocessing_function = preproc_fun_fixed)\n",
        "\n",
        "classes = ['0', '1', '2']\n",
        "save_dir = os.path.join(train_path, 'augmented')\n",
        "\n",
        "train_gen = train_data_gen.flow_from_directory(train_path,\n",
        "                                               target_size=(img_w, img_h),\n",
        "                                               seed=SEED,\n",
        "                                               classes=classes,\n",
        "                                               subset='training',\n",
        "                                               shuffle=True,\n",
        "                                               batch_size=bs\n",
        "                                               )\n",
        "\n",
        "valid_gen = train_data_gen.flow_from_directory(train_path,\n",
        "                                               target_size=(img_w, img_h),\n",
        "                                               seed=SEED,\n",
        "                                               classes=classes,\n",
        "                                               subset='validation',\n",
        "                                               shuffle=False,\n",
        "                                               batch_size=bs\n",
        "                                             )\n",
        "#new\n",
        "import pandas as pd\n",
        "images = [f for f in os.listdir(test_path)]\n",
        "images = pd.DataFrame(images)\n",
        "images.rename(columns = {0:'filename'}, inplace = True)\n",
        "images[\"class\"] = 'test'\n",
        "\n",
        "test_gen = test_data_gen.flow_from_dataframe(images,\n",
        "                                               test_path,\n",
        "                                               batch_size=bs,\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               class_mode='categorical',\n",
        "                                               shuffle=False,\n",
        "                                               seed=SEED)\n",
        "\n",
        "\n",
        "\n",
        "#end new\n",
        "\n",
        "\n",
        "train_set = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                           output_types=(tf.float32, tf.float32),\n",
        "                                           output_shapes=(\n",
        "                                               [None, img_w, img_h, 3],\n",
        "                                               [None, len(classes)]\n",
        "                                           ))\n",
        "\n",
        "validation_set = tf.data.Dataset.from_generator(lambda: valid_gen,\n",
        "                                                output_types=(tf.float32, tf.float32),\n",
        "                                                output_shapes=(\n",
        "                                                    [None, img_w, img_h, 3],\n",
        "                                                    [None, len(classes)]\n",
        "                                                ))\n",
        "\n",
        "test_set = tf.data.Dataset.from_generator(lambda: test_gen,\n",
        "                                          output_types=(tf.float32, tf.float32),\n",
        "                                          output_shapes=(\n",
        "                                              [None, img_w, img_h, 3],\n",
        "                                              [None, len(classes)]\n",
        "                                          ))\n",
        "\n",
        "train_set.repeat()\n",
        "validation_set.repeat()\n",
        "test_set.repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mfy08d2RSax"
      },
      "source": [
        "callbacks = []\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights= True)\n",
        "    callbacks.append(es_callback)\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = os.getcwd() + '/drive/My Drive/weights_xc.h5',\n",
        "      verbose=1, save_best_only=True, save_weights_only=True)\n",
        "    callbacks.append(cp_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyExUFTXR7vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b89832-747f-4cee-9ca1-92d8f70a2ef7"
      },
      "source": [
        "# Create Model\n",
        "# ------------\n",
        "\n",
        "finetuning = True\n",
        "\n",
        "if finetuning:\n",
        "    freeze_until = 1 # layer from which we want to fine-tune\n",
        "    \n",
        "    for layer in Xception.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "else:\n",
        "    Xception.trainable = False\n",
        "    \n",
        "model = tf.keras.Sequential()\n",
        "model.add(Xception)\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units=1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(units=512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "# Visualize created model as a table\n",
        "model.summary()\n",
        "\n",
        "# Visualize initialized weights\n",
        "model.weights\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# maybe explore learning rate solutions\n",
        "lr = 1e-6\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "metrics = ['accuracy']\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "\n",
        "train = False\n",
        "retrain = False\n",
        "if train:\n",
        "  if retrain:\n",
        "    model.load_weights('/content/drive/My Drive/weights_xc.h5')\n",
        "  model.fit(x=train_set,\n",
        "            epochs=30,  #### set repeat in training dataset\n",
        "            steps_per_epoch=len(train_gen),\n",
        "            validation_data=validation_set,\n",
        "            validation_steps=len(valid_gen),\n",
        "            callbacks=callbacks)\n",
        "else:\n",
        "  model.load_weights('/content/drive/My Drive/weights_xc.h5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "xception (Functional)        (None, 8, 8, 2048)        20861480  \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1024)              134218752 \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 155,737,131\n",
            "Trainable params: 155,682,603\n",
            "Non-trainable params: 54,528\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9957\n",
            "Epoch 00001: val_loss did not improve from 0.52192\n",
            "123/123 [==============================] - 221s 2s/step - loss: 0.0807 - accuracy: 0.9957 - val_loss: 0.5295 - val_accuracy: 0.8907\n",
            "Epoch 2/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9980\n",
            "Epoch 00002: val_loss improved from 0.52192 to 0.50805, saving model to /content/drive/My Drive/weights_XC_copia.h5\n",
            "123/123 [==============================] - 225s 2s/step - loss: 0.0731 - accuracy: 0.9980 - val_loss: 0.5081 - val_accuracy: 0.9032\n",
            "Epoch 3/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9957\n",
            "Epoch 00003: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 222s 2s/step - loss: 0.0761 - accuracy: 0.9957 - val_loss: 0.5142 - val_accuracy: 0.8967\n",
            "Epoch 4/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9949\n",
            "Epoch 00004: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 222s 2s/step - loss: 0.0753 - accuracy: 0.9949 - val_loss: 0.5273 - val_accuracy: 0.8979\n",
            "Epoch 5/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9962\n",
            "Epoch 00005: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 221s 2s/step - loss: 0.0753 - accuracy: 0.9962 - val_loss: 0.5200 - val_accuracy: 0.8973\n",
            "Epoch 6/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9962\n",
            "Epoch 00006: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 221s 2s/step - loss: 0.0747 - accuracy: 0.9962 - val_loss: 0.5252 - val_accuracy: 0.8985\n",
            "Epoch 7/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9962\n",
            "Epoch 00007: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 221s 2s/step - loss: 0.0721 - accuracy: 0.9962 - val_loss: 0.5268 - val_accuracy: 0.8907\n",
            "Epoch 8/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9964\n",
            "Epoch 00008: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 223s 2s/step - loss: 0.0708 - accuracy: 0.9964 - val_loss: 0.5101 - val_accuracy: 0.8990\n",
            "Epoch 9/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9972\n",
            "Epoch 00009: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 225s 2s/step - loss: 0.0696 - accuracy: 0.9972 - val_loss: 0.5191 - val_accuracy: 0.8967\n",
            "Epoch 10/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9962\n",
            "Epoch 00010: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 224s 2s/step - loss: 0.0705 - accuracy: 0.9962 - val_loss: 0.5092 - val_accuracy: 0.8961\n",
            "Epoch 11/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9972\n",
            "Epoch 00011: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 223s 2s/step - loss: 0.0705 - accuracy: 0.9972 - val_loss: 0.5361 - val_accuracy: 0.8973\n",
            "Epoch 12/30\n",
            "123/123 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9969Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.50805\n",
            "123/123 [==============================] - 221s 2s/step - loss: 0.0704 - accuracy: 0.9969 - val_loss: 0.5090 - val_accuracy: 0.9038\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRzyJNCPSE7Q"
      },
      "source": [
        "def create_csv(results, results_dir='/content/drive/My Drive'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BeUfIolSH7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c0e136-f36e-476a-c8dd-0ef01d0b468d"
      },
      "source": [
        "test_gen.reset()\n",
        "predictions = model.predict_generator(test_gen, len(test_gen), verbose=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-c6e16a144968>:2: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.predict, which supports generators.\n",
            "15/15 [==============================] - 7s 461ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOwYEcnOSKcm"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "results = {}\n",
        "images = test_gen.filenames\n",
        "i = 0\n",
        "\n",
        "for p in predictions:\n",
        "  prediction = np.argmax(p)\n",
        "  import ntpath\n",
        "  image_name = ntpath.basename(images[i])\n",
        "  results[image_name] = str(prediction)\n",
        "  i = i + 1"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AGY9Bbgp78Q"
      },
      "source": [
        "create_csv(results)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c__zW1HGqDZg"
      },
      "source": [
        "# Save the model\n",
        "save = True\n",
        "if save:\n",
        "    model.save('/content/drive/My Drive/model_xc.h5')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5bdmMerBvty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}