{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "main.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nicologhielmetti/AN2DL-challenges/blob/master/custom_arch_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p-jkRdXZjSbx",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1Mv7vKoI-QL6kV-1TIDE7N67_L0LXvJAg\n",
    "!unzip /content/ANDL2.zip"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\r\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h    Preparing wheel metadata ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from gdown) (4.51.0)\r\n",
      "Collecting filelock\r\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\r\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from gdown) (1.15.0)\r\n",
      "Requirement already satisfied: requests[socks] in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from gdown) (2.24.0)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from requests[socks]->gdown) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from requests[socks]->gdown) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from requests[socks]->gdown) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from requests[socks]->gdown) (1.25.11)\r\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\r\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\r\n",
      "Building wheels for collected packages: gdown\r\n",
      "  Building wheel for gdown (PEP 517) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9681 sha256=86dcda712b90c12f822999c4f5bd3280dcdb22a1dc57924eb39bbd959e136b6d\r\n",
      "  Stored in directory: /home/nicolo/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\r\n",
      "Successfully built gdown\r\n",
      "Installing collected packages: filelock, gdown, PySocks\r\n",
      "Successfully installed PySocks-1.7.1 filelock-3.0.12 gdown-3.12.2\r\n",
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=1Mv7vKoI-QL6kV-1TIDE7N67_L0LXvJAg\r\n",
      "To: /home/nicolo/PycharmProjects/AN2DL-challenges/challenge1/ANDL2.zip\r\n",
      "7.86MB [00:07, 1.20MB/s]^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/bin/gdown\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages/gdown/cli.py\", line 105, in main\r\n",
      "    use_cookies=not args.no_cookies,\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages/gdown/download.py\", line 197, in download\r\n",
      "    for chunk in res.iter_content(chunk_size=CHUNK_SIZE):\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages/requests/models.py\", line 751, in generate\r\n",
      "    for chunk in self.raw.stream(chunk_size, decode_content=True):\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages/urllib3/response.py\", line 571, in stream\r\n",
      "    for line in self.read_chunked(amt, decode_content=decode_content):\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages/urllib3/response.py\", line 766, in read_chunked\r\n",
      "    chunk = self._handle_chunk(amt)\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages/urllib3/response.py\", line 714, in _handle_chunk\r\n",
      "    value = self._fp._safe_read(amt)\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/http/client.py\", line 624, in _safe_read\r\n",
      "    chunk = self.fp.read(min(amt, MAXAMOUNT))\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/socket.py\", line 589, in readinto\r\n",
      "    return self._sock.recv_into(b)\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/ssl.py\", line 1071, in recv_into\r\n",
      "    return self.read(nbytes, buffer)\r\n",
      "  File \"/usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/ssl.py\", line 929, in read\r\n",
      "    return self._sslobj.read(len, buffer)\r\n",
      "KeyboardInterrupt\r\n",
      "7.86MB [00:07, 1.06MB/s]\r\n",
      "unzip:  cannot find or open /content/ANDL2.zip, /content/ANDL2.zip.zip or /content/ANDL2.zip.ZIP.\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z-MbZP061uud",
    "outputId": "3e6075b0-29e6-4a9c-c921-9918a26faa61",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-b80391d2b2cf>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdrive\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdrive\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/content/drive'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "70TyR7ZHDScm",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorboard import program\n",
    "\n",
    "SEED = 1996\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t6FiWcenqn79",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "class ConvBlock(tf.keras.Model):\n",
    "    def __init__(self, num_filters, regularizer, use_batch_norm):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv2d = tf.keras.layers.Conv2D(filters=num_filters,\n",
    "                                             kernel_size=(3, 3),\n",
    "                                             strides=(1, 1),\n",
    "                                             padding='same',\n",
    "                                             kernel_regularizer=regularizer,\n",
    "                                             bias_regularizer=regularizer)\n",
    "        if use_batch_norm is True:\n",
    "            self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self.activation = tf.keras.layers.ReLU()\n",
    "        self.pooling = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.conv2d(inputs)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pooling(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNClassifier(tf.keras.Model):\n",
    "    def __init__(self, depth_conv, start_f, num_classes, init_neurons_fc=512,\n",
    "                 regularizer=None, dropout=None, depth_fc=4, use_batch_norm=True):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        self.feature_extractor = tf.keras.Sequential()\n",
    "\n",
    "        for _ in range(depth_conv):\n",
    "            self.feature_extractor.add(ConvBlock(num_filters=start_f, regularizer=regularizer,\n",
    "                                                 use_batch_norm=use_batch_norm))\n",
    "            start_f *= 2\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.classifier = tf.keras.Sequential()\n",
    "\n",
    "        for _ in range(depth_fc):\n",
    "          self.classifier.add(tf.keras.layers.Dense(units=init_neurons_fc, activation='relu',\n",
    "                                                    kernel_regularizer=regularizer, bias_regularizer=regularizer))\n",
    "          if dropout is not None:\n",
    "            self.classifier.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "        self.classifier.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.feature_extractor(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def summary(self, line_length=None, positions=None, print_fn=None):\n",
    "        #super(CNNClassifier, self).summary(line_length, positions, print_fn)\n",
    "        self.feature_extractor.summary()\n",
    "        self.classifier.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FulVNpNKqPoG",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "def divideDatasetInTargetFolders(json_definition, dataset_path):\n",
    "    for elem in json_definition:\n",
    "        dest_dir = os.path.join(dataset_path, str(json_definition[elem]))\n",
    "        if not os.path.isdir(dest_dir):\n",
    "            os.mkdir(dest_dir)\n",
    "        try:\n",
    "            shutil.move(os.path.join(dataset_path, elem),\n",
    "                        os.path.join(dest_dir, elem)\n",
    "                        )\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"File not found: \" + str(e))\n",
    "            continue\n",
    "    os.mkdir(os.path.join(dataset_path, \"augmented\"))\n",
    "    os.mkdir(os.path.join(dataset_path, \"augmented/training\"))\n",
    "    os.mkdir(os.path.join(dataset_path, \"augmented/validation\"))\n",
    "\n",
    "\n",
    "def getMaxImageSize(dataset_dir):\n",
    "    max_w = 0\n",
    "    max_h = 0\n",
    "    path = os.path.join(os.getcwd(), dataset_dir)\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image = Image.open(os.path.join(path, filename))\n",
    "            width, height = image.size\n",
    "            max_w = width if width > max_w else max_w\n",
    "            max_h = height if height > max_h else max_h\n",
    "        else:\n",
    "            print(\"This file -> \" + filename + \" is not .jpg\")\n",
    "    return max_w, max_h\n",
    "\n",
    "\n",
    "def getMinImageSize(dataset_dir, max_w, max_h):\n",
    "    min_w = max_w\n",
    "    min_h = max_h\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image = Image.open(os.path.join(dataset_dir, filename))\n",
    "            width, height = image.size\n",
    "            min_w = width if width < min_w else min_w\n",
    "            min_h = height if height < min_h else min_h\n",
    "        else:\n",
    "            print(\"This file -> \" + filename + \" is not .jpg\")\n",
    "    return min_w, min_h"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PgeZo91gqUEe",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "train_path = os.path.join(os.getcwd(), 'MaskDataset/training')\n",
    "test_path  = os.path.join(os.getcwd(), 'MaskDataset/test')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m9sBYyixqZIL",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "division_dict = json.load(\n",
    "  open(os.path.join(os.getcwd(), 'MaskDataset/train_gt.json'))\n",
    ")\n",
    "\n",
    "divideDatasetInTargetFolders(division_dict, train_path)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VK0d9f1gqhla",
    "outputId": "467ed90f-533d-4cb9-d863-f564d1786588",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "# remember to check both train and test datasets to be sure of max dimensions\n",
    "max_w, max_h = max(getMaxImageSize(os.path.join(train_path, '0')),\n",
    "                   getMaxImageSize(os.path.join(train_path, '1')),\n",
    "                   getMaxImageSize(os.path.join(train_path, '2')))\n",
    "print(\"Maximum width and height: \" + str((max_w, max_h)))\n",
    "\n",
    "min_w, min_h = min(getMinImageSize(os.path.join(train_path, '0'), max_w, max_h),\n",
    "                   getMinImageSize(os.path.join(train_path, '1'), max_w, max_h),\n",
    "                   getMinImageSize(os.path.join(train_path, '2'), max_w, max_h))\n",
    "print(\"Minimum width and height:  \" + str((min_w, min_h)))\n",
    "print(\"Maximum width  expansion:  \" + str(max_w - min_w) + \", increase ratio: \" +\n",
    "      str(float(max_w) / float(max_w - min_w)))\n",
    "print(\"Maximum height expansion:  \" + str(max_h - min_h) + \", increase ratio: \" +\n",
    "      str(float(max_h) / float(max_h - min_h)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F38AucnZqid9",
    "outputId": "d955efcb-653a-46ef-fe60-b1aca55c692d",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "preproc_fun_fixed = partial(tf.keras.preprocessing.image.smart_resize, size=(max_w, max_h))\n",
    "\n",
    "train_data_gen = ImageDataGenerator(rotation_range=10,\n",
    "                                    width_shift_range=10,\n",
    "                                    height_shift_range=10,\n",
    "                                    zoom_range=0.3,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode='reflect',\n",
    "                                    rescale=1. / 255,\n",
    "                                    validation_split=0.3,\n",
    "                                    preprocessing_function=preproc_fun_fixed\n",
    "                                    )\n",
    "\n",
    "test_data_gen = ImageDataGenerator(rescale=1. / 255, preprocessing_function=preproc_fun_fixed)\n",
    "\n",
    "classes = ['0', '1', '2']\n",
    "save_dir = os.path.join(train_path, 'augmented')\n",
    "\n",
    "import pandas as pd\n",
    "images = [f for f in os.listdir(test_path)]\n",
    "images = pd.DataFrame(images)\n",
    "images.rename(columns = {0:'filename'}, inplace = True)\n",
    "images[\"class\"] = 'test'\n",
    "\n",
    "bs = 32\n",
    "\n",
    "train_gen = train_data_gen.flow_from_directory(train_path,\n",
    "                                               target_size=(max_w, max_h),\n",
    "                                               seed=SEED,\n",
    "                                               classes=classes,\n",
    "                                               #save_prefix='training_aug',\n",
    "                                               #save_to_dir=os.path.join(save_dir, 'training'),\n",
    "                                               subset='training',\n",
    "                                               shuffle=True,\n",
    "                                               batch_size=bs\n",
    "                                               )\n",
    "\n",
    "valid_gen = train_data_gen.flow_from_directory(train_path,\n",
    "                                               target_size=(max_w, max_h),\n",
    "                                               seed=SEED,\n",
    "                                               classes=classes,\n",
    "                                               #save_prefix='validation',\n",
    "                                               #save_to_dir=os.path.join(save_dir, 'validation'),\n",
    "                                               subset='validation',\n",
    "                                               shuffle=False,\n",
    "                                               batch_size=bs\n",
    "                                               )\n",
    "\n",
    "test_gen = test_data_gen.flow_from_dataframe(images,\n",
    "                                               test_path,\n",
    "                                               batch_size=bs,\n",
    "                                               target_size=(max_h, max_w),\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=False,\n",
    "                                               seed=SEED)\n",
    "\n",
    "# set the right order for predictions\n",
    "test_gen.reset()\n",
    "\n",
    "train_set = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                           output_types=(tf.float32, tf.float32),\n",
    "                                           output_shapes=(\n",
    "                                               [None, max_w, max_h, 3],\n",
    "                                               [None, len(classes)]\n",
    "                                           ))\n",
    "\n",
    "validation_set = tf.data.Dataset.from_generator(lambda: valid_gen,\n",
    "                                                output_types=(tf.float32, tf.float32),\n",
    "                                                output_shapes=(\n",
    "                                                    [None, max_w, max_h, 3],\n",
    "                                                    [None, len(classes)]\n",
    "                                                ))\n",
    "\n",
    "test_set = tf.data.Dataset.from_generator(lambda: test_gen,\n",
    "                                          output_types=(tf.float32, tf.float32),\n",
    "                                          output_shapes=(\n",
    "                                              [None, max_w, max_h, 3],\n",
    "                                              [None, len(classes)]\n",
    "                                          ))\n",
    "\n",
    "train_set.repeat()\n",
    "validation_set.repeat()\n",
    "test_set.repeat()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WOqm4Q4Oqyr7",
    "outputId": "09fe013b-3344-4f6b-f858-54b5dfcf7392",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "start_f = 6\n",
    "depth_conv = 6\n",
    "depth_fc = 4\n",
    "\n",
    "model = CNNClassifier(depth_conv=depth_conv,\n",
    "                      start_f=start_f,\n",
    "                      num_classes=len(classes),\n",
    "                      regularizer=tf.keras.regularizers.l1(),\n",
    "                      dropout=0.4,\n",
    "                      depth_fc=depth_fc\n",
    "                      )\n",
    "\n",
    "model.build(input_shape=(None, max_h, max_w, 3))\n",
    "\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tGfrCpzbrfwL",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "callbacks = []\n",
    "tensorboard = False\n",
    "if tensorboard:\n",
    "  tracking_address = os.path.join(os.getcwd(), \"tracking_dir\")\n",
    "  tb = program.TensorBoard()\n",
    "  tb.configure(argv=[None, '--logdir', tracking_address])\n",
    "  url = tb.launch()\n",
    "\n",
    "  if not os.path.exists(tracking_address):\n",
    "      os.makedirs(tracking_address)\n",
    "\n",
    "  now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "  model_name = 'CNN'\n",
    "\n",
    "  exp_dir = os.path.join(tracking_address, model_name + '_' + str(now))\n",
    "  if not os.path.exists(exp_dir):\n",
    "      os.makedirs(exp_dir)\n",
    "\n",
    "  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "  if not os.path.exists(ckpt_dir):\n",
    "      os.makedirs(ckpt_dir)\n",
    "\n",
    "  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'),\n",
    "                                                    save_weights_only=True)  # False to save the model directly\n",
    "  callbacks.append(ckpt_callback)\n",
    "\n",
    "  tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "  if not os.path.exists(tb_dir):\n",
    "      os.makedirs(tb_dir)\n",
    "\n",
    "  # By default shows losses and metrics for both training and validation\n",
    "  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                              profile_batch=0,\n",
    "                                              histogram_freq=1)  # if 1 shows weights histograms\n",
    "  callbacks.append(tb_callback)\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir /content/tracking_dir"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "daD1-vy-urJF",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "early_stop = True\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = os.getcwd() + '/drive/My Drive/weights_nik.h5',\n",
    "      verbose=1, save_best_only=True, save_weights_only=False)\n",
    "    callbacks.append(cp_callback)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UPYQd2vQvp8R",
    "colab": {
     "background_save": true
    }
   },
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# maybe explore learning rate solutions\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8tC-b2uorkO0",
    "outputId": "809bed5a-785d-4d41-f214-109db4da535b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "train = True\n",
    "retrain = True\n",
    "if train:\n",
    "  if retrain:\n",
    "    model.load_weights('/content/drive/My Drive/weights_nik.h5')\n",
    "  model.fit(x=train_set,\n",
    "            epochs=100,  #### set repeat in training dataset\n",
    "            steps_per_epoch=len(train_gen),\n",
    "            validation_data=validation_set,\n",
    "            validation_steps=len(valid_gen),\n",
    "            callbacks=callbacks)\n",
    "else:\n",
    "  model.load_weights('/content/drive/My Drive/weights_nik.h5')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sxal599l3y_w"
   },
   "source": [
    "#testing"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N5AUdikqPz10"
   },
   "source": [
    "def create_csv(results, results_dir='/content/drive/My Drive'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MaxCtSCLP7ss"
   },
   "source": [
    "predictions = model.predict_generator(test_gen, len(test_gen), verbose=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ocOXozSxQALv"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "results = {}\n",
    "images = test_gen.filenames\n",
    "i = 0\n",
    "\n",
    "for p in predictions:\n",
    "  prediction = np.argmax(p)\n",
    "  import ntpath\n",
    "  image_name = ntpath.basename(images[i])\n",
    "  results[image_name] = str(prediction)\n",
    "  i = i + 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yPg5kdscQCpz"
   },
   "source": [
    "create_csv(results)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hh_M3BlGQDS9"
   },
   "source": [
    "!cp results_Nov15_11-49-28.csv \"/content/drive/My Drive/\""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}