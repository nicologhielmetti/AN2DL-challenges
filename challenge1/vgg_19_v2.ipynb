{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "vgg.ipynb",
   "provenance": [],
   "authorship_tag": "ABX9TyP9h8C2gwQjH391hevfTQpJ"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/nicologhielmetti/AN2DL-challenges/blob/master/challenge1/vgg_19_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "axkVKeQdPt2r",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1605600953476,
     "user_tz": -60,
     "elapsed": 28264,
     "user": {
      "displayName": "Lorenzo Amaranto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgQ3f9RW4iaTlZ_nIF8w2hyc3vBclrqPT6Lm_fFUVI=s64",
      "userId": "08097200368058693465"
     }
    },
    "outputId": "5e56842f-31f9-4e6c-8693-a6efa5c2e6c0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1Mv7vKoI-QL6kV-1TIDE7N67_L0LXvJAg\n",
    "!unzip /content/ANDL2.zip"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (3.12.2)\r\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from gdown) (1.15.0)\r\n",
      "Requirement already satisfied: requests[socks] in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from gdown) (2.24.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from gdown) (4.51.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from gdown) (3.0.12)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from requests[socks]->gdown) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from requests[socks]->gdown) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from requests[socks]->gdown) (1.25.11)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from requests[socks]->gdown) (2.10)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/anaconda3/envs/AN2DL-kaggle/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\r\n",
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=1Mv7vKoI-QL6kV-1TIDE7N67_L0LXvJAg\r\n",
      "To: /home/nicolo/PycharmProjects/AN2DL-challenges/challenge1/ANDL2.zip\r\n",
      "433MB [09:40, 745kB/s]  \r\n",
      "unzip:  cannot find or open /content/ANDL2.zip, /content/ANDL2.zip.zip or /content/ANDL2.zip.ZIP.\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-b80391d2b2cf>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdrive\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdrive\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/content/drive'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorboard import program\n",
    "\n",
    "SEED = 1996"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def divideDatasetInTargetFolders(json_definition, dataset_path):\n",
    "    for elem in json_definition:\n",
    "        dest_dir = os.path.join(dataset_path, str(json_definition[elem]))\n",
    "        if not os.path.isdir(dest_dir):\n",
    "            os.mkdir(dest_dir)\n",
    "        try:\n",
    "            shutil.move(os.path.join(dataset_path, elem),\n",
    "                        os.path.join(dest_dir, elem)\n",
    "                        )\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"File not found: \" + str(e))\n",
    "            continue\n",
    "    os.mkdir(os.path.join(dataset_path, \"augmented\"))\n",
    "    os.mkdir(os.path.join(dataset_path, \"augmented/training\"))\n",
    "    os.mkdir(os.path.join(dataset_path, \"augmented/validation\"))\n",
    "\n",
    "\n",
    "def getMaxImageSize(dataset_dir):\n",
    "    max_w = 0\n",
    "    max_h = 0\n",
    "    path = os.path.join(os.getcwd(), dataset_dir)\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image = Image.open(os.path.join(path, filename))\n",
    "            width, height = image.size\n",
    "            max_w = width if width > max_w else max_w\n",
    "            max_h = height if height > max_h else max_h\n",
    "        else:\n",
    "            print(\"This file -> \" + filename + \" is not .jpg\")\n",
    "    return max_w, max_h\n",
    "\n",
    "\n",
    "def getMinImageSize(dataset_dir, max_w, max_h):\n",
    "    min_w = max_w\n",
    "    min_h = max_h\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image = Image.open(os.path.join(dataset_dir, filename))\n",
    "            width, height = image.size\n",
    "            min_w = width if width < min_w else min_w\n",
    "            min_h = height if height < min_h else min_h\n",
    "        else:\n",
    "            print(\"This file -> \" + filename + \" is not .jpg\")\n",
    "    return min_w, min_h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_path = os.path.join(os.getcwd(), 'MaskDataset/training')\n",
    "test_path  = os.path.join(os.getcwd(), 'MaskDataset/test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "division_dict = json.load(\n",
    "  open(os.path.join(os.getcwd(), 'MaskDataset/train_gt.json'))\n",
    ")\n",
    "\n",
    "divideDatasetInTargetFolders(division_dict, train_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remember to check both train and test datasets to be sure of max dimensions\n",
    "max_w, max_h = max(getMaxImageSize(os.path.join(train_path, '0')),\n",
    "                   getMaxImageSize(os.path.join(train_path, '1')),\n",
    "                   getMaxImageSize(os.path.join(train_path, '2')))\n",
    "print(\"Maximum width and height: \" + str((max_w, max_h)))\n",
    "\n",
    "min_w, min_h = min(getMinImageSize(os.path.join(train_path, '0'), max_w, max_h),\n",
    "                   getMinImageSize(os.path.join(train_path, '1'), max_w, max_h),\n",
    "                   getMinImageSize(os.path.join(train_path, '2'), max_w, max_h))\n",
    "print(\"Minimum width and height:  \" + str((min_w, min_h)))\n",
    "print(\"Maximum width  expansion:  \" + str(max_w - min_w) + \", increase ratio: \" +\n",
    "      str(float(max_w) / float(max_w - min_w)))\n",
    "print(\"Maximum height expansion:  \" + str(max_h - min_h) + \", increase ratio: \" +\n",
    "      str(float(max_h) / float(max_h - min_h)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preproc_fun_fixed = partial(tf.keras.preprocessing.image.smart_resize, size=(max_w, max_h))\n",
    "\n",
    "train_data_gen = ImageDataGenerator(rotation_range=10,\n",
    "                                    width_shift_range=10,\n",
    "                                    height_shift_range=10,\n",
    "                                    zoom_range=0.3,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode='reflect',\n",
    "                                    rescale=1. / 255,\n",
    "                                    validation_split=0.3,\n",
    "                                    preprocessing_function=preproc_fun_fixed\n",
    "                                    )\n",
    "\n",
    "test_data_gen = ImageDataGenerator(rescale=1. / 255, preprocessing_function=preproc_fun_fixed)\n",
    "\n",
    "classes = ['0', '1', '2']\n",
    "save_dir = os.path.join(train_path, 'augmented')\n",
    "\n",
    "import pandas as pd\n",
    "images = [f for f in os.listdir(test_path)]\n",
    "images = pd.DataFrame(images)\n",
    "images.rename(columns = {0:'filename'}, inplace = True)\n",
    "images[\"class\"] = 'test'\n",
    "\n",
    "bs = 32\n",
    "\n",
    "train_gen = train_data_gen.flow_from_directory(train_path,\n",
    "                                               target_size=(max_w, max_h),\n",
    "                                               seed=SEED,\n",
    "                                               classes=classes,\n",
    "                                               #save_prefix='training_aug',\n",
    "                                               #save_to_dir=os.path.join(save_dir, 'training'),\n",
    "                                               subset='training',\n",
    "                                               shuffle=True,\n",
    "                                               batch_size=bs\n",
    "                                               )\n",
    "\n",
    "valid_gen = train_data_gen.flow_from_directory(train_path,\n",
    "                                               target_size=(max_w, max_h),\n",
    "                                               seed=SEED,\n",
    "                                               classes=classes,\n",
    "                                               #save_prefix='validation',\n",
    "                                               #save_to_dir=os.path.join(save_dir, 'validation'),\n",
    "                                               subset='validation',\n",
    "                                               shuffle=False,\n",
    "                                               batch_size=bs\n",
    "                                               )\n",
    "\n",
    "test_gen = test_data_gen.flow_from_dataframe(images,\n",
    "                                               test_path,\n",
    "                                               batch_size=bs,\n",
    "                                               target_size=(max_h, max_w),\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=False,\n",
    "                                               seed=SEED)\n",
    "\n",
    "# set the right order for predictions\n",
    "test_gen.reset()\n",
    "\n",
    "train_set = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                           output_types=(tf.float32, tf.float32),\n",
    "                                           output_shapes=(\n",
    "                                               [None, max_w, max_h, 3],\n",
    "                                               [None, len(classes)]\n",
    "                                           ))\n",
    "\n",
    "validation_set = tf.data.Dataset.from_generator(lambda: valid_gen,\n",
    "                                                output_types=(tf.float32, tf.float32),\n",
    "                                                output_shapes=(\n",
    "                                                    [None, max_w, max_h, 3],\n",
    "                                                    [None, len(classes)]\n",
    "                                                ))\n",
    "\n",
    "test_set = tf.data.Dataset.from_generator(lambda: test_gen,\n",
    "                                          output_types=(tf.float32, tf.float32),\n",
    "                                          output_shapes=(\n",
    "                                              [None, max_w, max_h, 3],\n",
    "                                              [None, len(classes)]\n",
    "                                          ))\n",
    "\n",
    "train_set.repeat()\n",
    "validation_set.repeat()\n",
    "test_set.repeat()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(max_h, max_w, 3))\n",
    "vgg.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}