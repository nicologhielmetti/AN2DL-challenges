{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "challenge_1_submission.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicologhielmetti/AN2DL-challenges/blob/master/challenge1/challenge_1_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bGLZVzosVOx"
      },
      "source": [
        "# Data Preprocessing\n",
        "The dataset provided for this challenge is composed by 6065 images of different height and width. In order to obtain a standard shape for all images, a particular function we used, namely `tf.keras.preprocessing.image.smart_resize()`.\n",
        "\n",
        "This function resizes images to a specified target size without affecting the aspect ratio. This step is due to the successive batching of all images that requires a common format for all of them.\n",
        "In order to avoid a loss of information that would have been caused by applying a crop, it has been computed what the max width/height of the images was by implementing `getMaxImageSize(dataset_dir)`.\n",
        "\n",
        "This function returns the maximum (width,height), considering all the images of the provided dataset.\n",
        "We also considered different shapes ranging from the mean and a standard one (256x256).\n",
        "For what regards data augmentation and the splitting of the data into two distinct sets, namely train and validation, it has been used the following script which assigns a fraction of 30% of the imgaes to the validation set; the rest will be assigned to the training set.\n",
        "\n",
        "```\n",
        "train_data_gen = ImageDataGenerator ( rotation_range=10,            \n",
        "                                      width_shift_range=10,\n",
        "                                      height_shift_range=10,\n",
        "                                      zoom_range=0.3, \n",
        "                                      horizontal_flip=True,\n",
        "                                      fill_mode='reflect',\n",
        "                                      rescale=1. / 255,\n",
        "                                      validation_split=0.3,  \n",
        "                                      preproc_funct=smart_resize()  \n",
        "                                    )\n",
        "```\n",
        "Considering that a strict division of the images in subdirectories representing the target classes is required for the functions involved in the creation of augmented images, we implemented a specific function that requires a ```json_definition``` for the subdirectory construction and the path where such images are located:\n",
        "\n",
        "\n",
        "# Model Design\n",
        "\n",
        "Two main different approaches have been used in order to address the problem proposed for this challenge: the creation of custom models from scratch and the exploitation of already existing models with transfer learning and fine tuning. In the former solution we started with a very simple network composed only by a sequence of convolutional layers and relu activation functions. To overcome the poor results obtained it has been decided to increase the complexity of the model all along with different regularization procedures such as dropout and l2regularization. Despite our effort it has not been possible to achieve a satisfying score. So we moved to more suitable solutions that is fine tuning and transfer learning. Several different architectures have been tried with an increasing level of acceptability of the results in the range between 80 and 89% with respect to the validation accuracy. We noticed that with larger classifiers put on top of the backbone network a better val accuracy was achieved but we also noticed the divergence of the loss on the train and validation sets; this fact is a sign of overfitting and should be taken into account in the model selection phase. We perfomed a wide exploration for what regards learning rates in order to overcome the problem of local minima in the loss minimization.\n",
        "\n",
        "\n",
        "# Model Selection\n",
        "\n",
        "To select the best model among all the designed ones we firstly considered the score on the validation set. But this metric is generally not enough to choose the model that generalize better over the test set. To overcome this problem we considered also the difference between the validation and the training loss. Considering also this metric we chose five candidates for the submission on kaggle. Among the scores obtained with those models we picked the one with the highest public score for the final submission.\n",
        "\n",
        "#Results\n",
        "The best model resulted to be ResNet50V2 with a score of 92% over the test set.\n",
        "\n",
        "### The following part will be dedicated to show the implementation of our best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yM6rPLn_LpXi"
      },
      "source": [
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1Mv7vKoI-QL6kV-1TIDE7N67_L0LXvJAg\n",
        "!gdown https://drive.google.com/uc?id=1-CNDi845KMDqrqiuIjAepo9MtoiEMg9o\n",
        "!unzip /content/ANDL2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "o9IGyh-oLpXi"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorboard import program\n",
        "\n",
        "SEED = 1996"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tdcwCWYrLpXi"
      },
      "source": [
        "def divideDatasetInTargetFolders(json_definition, dataset_path):\n",
        "    for elem in json_definition:\n",
        "        dest_dir = os.path.join(dataset_path, str(json_definition[elem]))\n",
        "        if not os.path.isdir(dest_dir):\n",
        "            os.mkdir(dest_dir)\n",
        "        try:\n",
        "            shutil.move(os.path.join(dataset_path, elem),\n",
        "                        os.path.join(dest_dir, elem)\n",
        "                        )\n",
        "        except FileNotFoundError as e:\n",
        "            print(\"File not found: \" + str(e))\n",
        "            continue\n",
        "    os.mkdir(os.path.join(dataset_path, \"augmented\"))\n",
        "    os.mkdir(os.path.join(dataset_path, \"augmented/training\"))\n",
        "    os.mkdir(os.path.join(dataset_path, \"augmented/validation\"))\n",
        "\n",
        "\n",
        "def getMaxImageSize(dataset_dir):\n",
        "    max_w = 0\n",
        "    max_h = 0\n",
        "    path = os.path.join(os.getcwd(), dataset_dir)\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            image = Image.open(os.path.join(path, filename))\n",
        "            width, height = image.size\n",
        "            max_w = width if width > max_w else max_w\n",
        "            max_h = height if height > max_h else max_h\n",
        "        else:\n",
        "            print(\"This file -> \" + filename + \" is not .jpg\")\n",
        "    return max_w, max_h\n",
        "\n",
        "\n",
        "def getMinImageSize(dataset_dir, max_w, max_h):\n",
        "    min_w = max_w\n",
        "    min_h = max_h\n",
        "    for filename in os.listdir(dataset_dir):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            image = Image.open(os.path.join(dataset_dir, filename))\n",
        "            width, height = image.size\n",
        "            min_w = width if width < min_w else min_w\n",
        "            min_h = height if height < min_h else min_h\n",
        "        else:\n",
        "            print(\"This file -> \" + filename + \" is not .jpg\")\n",
        "    return min_w, min_h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DB9ALl-DLpXi"
      },
      "source": [
        "train_path = os.path.join(os.getcwd(), 'MaskDataset/training')\n",
        "test_path  = os.path.join(os.getcwd(), 'MaskDataset/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JB8WFoorLpXi"
      },
      "source": [
        "division_dict = json.load(\n",
        "  open(os.path.join(os.getcwd(), 'MaskDataset/train_gt.json'))\n",
        ")\n",
        "\n",
        "divideDatasetInTargetFolders(division_dict, train_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-gk2c_WcLpXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57652195-5e94-4f7a-86ae-6e5ad19c7658"
      },
      "source": [
        "max_w, max_h = max(getMaxImageSize(os.path.join(train_path, '0')),\n",
        "                   getMaxImageSize(os.path.join(train_path, '1')),\n",
        "                   getMaxImageSize(os.path.join(train_path, '2')))\n",
        "print(\"Maximum width and height: \" + str((max_w, max_h)))\n",
        "\n",
        "min_w, min_h = min(getMinImageSize(os.path.join(train_path, '0'), max_w, max_h),\n",
        "                   getMinImageSize(os.path.join(train_path, '1'), max_w, max_h),\n",
        "                   getMinImageSize(os.path.join(train_path, '2'), max_w, max_h))\n",
        "print(\"Minimum width and height:  \" + str((min_w, min_h)))\n",
        "print(\"Maximum width  expansion:  \" + str(max_w - min_w) + \", increase ratio: \" +\n",
        "      str(float(min_w) / float(max_w - min_w)))\n",
        "print(\"Maximum height expansion:  \" + str(max_h - min_h) + \", increase ratio: \" +\n",
        "      str(float(min_h) / float(max_h - min_h)))\n",
        "img_w = int(min_w + (max_w - min_w) / 2)\n",
        "img_h = int(min_h + (max_h - min_h) / 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum width and height: (612, 612)\n",
            "Minimum width and height:  (345, 325)\n",
            "Maximum width  expansion:  267, increase ratio: 1.2921348314606742\n",
            "Maximum height expansion:  287, increase ratio: 1.132404181184669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nTOqfuGZLpXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9744c9ba-d1f6-4d29-c1e5-e70dc4c28efe"
      },
      "source": [
        "preproc_fun_fixed = partial(tf.keras.preprocessing.image.smart_resize, size=(img_w, img_h))\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rotation_range=10,\n",
        "                                    width_shift_range=10,\n",
        "                                    height_shift_range=10,\n",
        "                                    zoom_range=0.3,\n",
        "                                    horizontal_flip=True,\n",
        "                                    fill_mode='reflect',\n",
        "                                    rescale=1. / 255,\n",
        "                                    validation_split=0.3,\n",
        "                                    preprocessing_function=preproc_fun_fixed\n",
        "                                    )\n",
        "\n",
        "test_data_gen = ImageDataGenerator(rotation_range=10,\n",
        "                                    width_shift_range=10,\n",
        "                                    height_shift_range=10,\n",
        "                                    zoom_range=0.3,\n",
        "                                    horizontal_flip=True,\n",
        "                                    fill_mode='reflect',\n",
        "                                    rescale=1. / 255,\n",
        "                                    preprocessing_function=preproc_fun_fixed\n",
        "                                  )\n",
        "\n",
        "classes = ['0', '1', '2']\n",
        "save_dir = os.path.join(train_path, 'augmented')\n",
        "\n",
        "import pandas as pd\n",
        "images = [f for f in os.listdir(test_path)]\n",
        "images = pd.DataFrame(images)\n",
        "images.rename(columns = {0:'filename'}, inplace = True)\n",
        "images[\"class\"] = 'test'\n",
        "\n",
        "bs = 32\n",
        "\n",
        "train_gen = train_data_gen.flow_from_directory(train_path,\n",
        "                                               target_size=(img_w, img_h),\n",
        "                                               seed=SEED,\n",
        "                                               classes=classes,\n",
        "                                               #save_prefix='training_aug',\n",
        "                                               #save_to_dir=os.path.join(save_dir, 'training'),\n",
        "                                               subset='training',\n",
        "                                               shuffle=True,\n",
        "                                               batch_size=bs\n",
        "                                               )\n",
        "\n",
        "valid_gen = train_data_gen.flow_from_directory(train_path,\n",
        "                                               target_size=(img_w, img_h),\n",
        "                                               seed=SEED,\n",
        "                                               classes=classes,\n",
        "                                               #save_prefix='validation',\n",
        "                                               #save_to_dir=os.path.join(save_dir, 'validation'),\n",
        "                                               subset='validation',\n",
        "                                               shuffle=False,\n",
        "                                               batch_size=bs\n",
        "                                               )\n",
        "\n",
        "test_gen = test_data_gen.flow_from_dataframe(images,\n",
        "                                             test_path,\n",
        "                                             batch_size=bs,\n",
        "                                             target_size=(img_w, img_h),\n",
        "                                             class_mode='categorical',\n",
        "                                             shuffle=False,\n",
        "                                             seed=SEED\n",
        "                                            )\n",
        "\n",
        "# set the right order for predictions\n",
        "test_gen.reset()\n",
        "\n",
        "train_set = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                           output_types=(tf.float32, tf.float32),\n",
        "                                           output_shapes=(\n",
        "                                               [None, img_w, img_h, 3],\n",
        "                                               [None, len(classes)]\n",
        "                                           ))\n",
        "\n",
        "validation_set = tf.data.Dataset.from_generator(lambda: valid_gen,\n",
        "                                                output_types=(tf.float32, tf.float32),\n",
        "                                                output_shapes=(\n",
        "                                                    [None, img_w, img_h, 3],\n",
        "                                                    [None, len(classes)]\n",
        "                                                ))\n",
        "\n",
        "test_set = tf.data.Dataset.from_generator(lambda: test_gen,\n",
        "                                          output_types=(tf.float32, tf.float32),\n",
        "                                          output_shapes=(\n",
        "                                              [None, img_w, img_h, 3],\n",
        "                                              [None, len(classes)]\n",
        "                                          ))\n",
        "\n",
        "train_set.repeat()\n",
        "validation_set.repeat()\n",
        "test_set.repeat()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3930 images belonging to 3 classes.\n",
            "Found 1684 images belonging to 3 classes.\n",
            "Found 450 validated image filenames belonging to 1 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<RepeatDataset shapes: ((None, 478, 468, 3), (None, 3)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "64MeFzLZLpXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ef447b-ad43-4a03-ab83-21e00104805f"
      },
      "source": [
        "resnet = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(img_w, img_h, 3))\n",
        "\n",
        "model_resnet = tf.keras.Sequential()\n",
        "model_resnet.add(resnet)\n",
        "model_resnet.add(tf.keras.layers.Flatten())\n",
        "model_resnet.add(tf.keras.layers.Dense(units=64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)))\n",
        "model_resnet.add(tf.keras.layers.Dense(units=32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)))\n",
        "model_resnet.add(tf.keras.layers.Dense(units=len(classes), activation='softmax'))\n",
        "\n",
        "model_resnet.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 9s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50v2 (Functional)      (None, 15, 15, 2048)      23564800  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 460800)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                29491264  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 53,058,243\n",
            "Trainable params: 53,012,803\n",
            "Non-trainable params: 45,440\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tSz43VgPLpXi"
      },
      "source": [
        "callbacks = []\n",
        "early_stop = False\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "    callbacks.append(es_callback)\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = weights_path,\n",
        "      verbose=1, save_best_only=True, save_weights_only=True)\n",
        "    callbacks.append(cp_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KWykj4kTLpXi"
      },
      "source": [
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# we explored different learning rates\n",
        "lr = 5e-5\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "metrics = ['accuracy']\n",
        "model_resnet.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KtEECk45LpXi"
      },
      "source": [
        "train = False\n",
        "retrain = False\n",
        "if train:\n",
        "  if retrain:\n",
        "    model_resnet.load_weights(os.getcwd() + '/weights_resnet_50_v2_ft_avg_img_size_fc3_smaller.h5')\n",
        "  model_resnet.fit(x=train_set,\n",
        "            epochs=100,  #### set repeat in training dataset\n",
        "            steps_per_epoch=len(train_gen),\n",
        "            validation_data=validation_set,\n",
        "            validation_steps=len(valid_gen),\n",
        "            callbacks=callbacks)\n",
        "else:\n",
        "  model_resnet.load_weights(os.getcwd() + '/weights_resnet_50_v2_ft_avg_img_size_fc3_smaller.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}