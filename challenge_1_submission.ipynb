{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "challenge_1_submission.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQo2XdDGxDJ5TMXcwnW8TL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicologhielmetti/AN2DL-challenges/blob/master/challenge_1_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj2Nr4tkqYwL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bGLZVzosVOx"
      },
      "source": [
        "# Data Preprocessing\n",
        "The dataset provided for this challenge is composed by 6065 images of different height and width. In order to obtain a standard shape for all images, a particular function we used, namely `tf.keras.preprocessing.image.smart_resize()`.\n",
        "\n",
        "This function resizes images to a specified target size without affecting the aspect ratio. This step is due to the successive batching of all images that requires a common format for all of them.\n",
        "In order to avoid a loss of information that would have been caused by applying a crop, it has been computed what the max width/height of the images was by implementing `getMaxImageSize(dataset_dir)`.\n",
        "\n",
        "This function returns the maximum (width,height), considering all the images of the provided dataset.\n",
        "We also considered different shapes ranging from the mean and a standard one (256x256).\n",
        "For what regards data augmentation and the splitting of the data into two distinct sets, namely train and validation, it has been used the following script which assigns a fraction of 30% of the imgaes to the validation set; the rest will be assigned to the training set.\n",
        "\n",
        "```\n",
        "train_data_gen = ImageDataGenerator ( rotation_range=10,            \n",
        "                                      width_shift_range=10,\n",
        "                                      height_shift_range=10,\n",
        "                                      zoom_range=0.3, \n",
        "                                      horizontal_flip=True,\n",
        "                                      fill_mode='reflect',\n",
        "                                      rescale=1. / 255,\n",
        "                                      validation_split=0.3,  \n",
        "                                      preproc_funct=smart_resize()  \n",
        "                                    )\n",
        "```\n",
        "Considering that a strict division of the images in subdirectories representing the target classes is required for the functions involved in the creation of augmented images, we implemented a specific function that requires a ```json_definition``` for the subdirectory construction and the path where such images are located:\n",
        "\n",
        "\n",
        "# Model Design\n",
        "\n",
        "Two main different approaches have been used in order to address the problem proposed for this challenge: the creation of custom models from scratch and the exploitation of already existing models with transfer learning and fine tuning. In the former solution we started with a very simple network composed only by a sequence of convolutional layers and relu activation functions. To overcome the poor results obtained it has been decided to increase the complexity of the model all along with different regularization procedures such as dropout and l2regularization. Despite our effort it has not been possible to achieve a satisfying score. So we moved to more suitable solutions that is fine tuning and transfer learning. Several different architectures have been tried with an increasing level of acceptability of the results in the range between 80 and 89% with respect to the validation accuracy. We noticed that with larger classifiers put on top of the backbone network a better val accuracy was achieved but we also noticed the divergence of the loss on the train and validation sets; this fact is a sign of overfitting and should be taken into account in the model selection phase. We perfomed a wide exploration for what regards learning rates in order to overcome the problem of local minima in the loss minimization.\n",
        "\n",
        "\n",
        "# Model Selection\n",
        "\n",
        "To select the best model among all the designed ones we firstly considered the score on the validation set. But this metric is generally not enough to choose the model that generalize better over the test set. To overcome this problem we considered also the difference between the validation and the training loss. Considering also this metric we chose five candidates for the submission on kaggle. Among the scores obtained with those models we picked the one with the highest public score for the final submission.\n",
        "\n",
        "#Results\n",
        "The best model resulted to be ____ with a score of ____\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}